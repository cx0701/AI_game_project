using System;
using System.Collections.Generic;
using System.Reflection;
using Newtonsoft.Json;
using rc = Glitch9.RangeCheck;

namespace Glitch9.AIDevKit
{
    /// <summary>
    /// This class defines a flexible set of parameters that control how text is generated by a language model. 
    /// These options are entirely optional, 
    /// but tuning them allows for precise control over randomness, 
    /// token filtering, sampling behavior, and performance tuning.  
    /// </summary>  
    public class ModelOptions
    {
        // General generation options --------------------------------------------------

        /// <summary>
        /// Optional. Maximum number of tokens (range: [1, context_length)).
        /// </summary>
        public int? MaxTokens { get; set; }

        /// <summary>
        /// Random seed for reproducibility. Set to the same value for deterministic output.
        /// </summary>
        [JsonProperty("seed")] public int? Seed { get; set; }

        /// <summary>
        /// Top-K sampling: limits the next token selection to the K most probable tokens.
        /// Range: 1–100 (typical: 40).
        /// </summary>
        [JsonProperty("top_k")] public int? TopK { get; set; }

        /// <summary>
        /// Top-P sampling (nucleus sampling): limits the next token selection to a cumulative probability.
        /// Range: 0.0–1.0 (typical: 0.9).
        /// </summary>
        [JsonProperty("top_p")] public float? TopP { get; set; }

        /// <summary>
        /// Top-A sampling: limits the next token selection to the A most probable tokens.
        /// Range: 1–100 (typical: 40).
        /// </summary>
        public float? TopA { get; set; }

        /// <summary>
        /// Minimum probability for token filtering (less common).
        /// Range: 0.0–1.0
        /// </summary>
        [JsonProperty("min_p")] public float? MinP { get; set; }

        /// <summary>
        /// Sampling temperature: controls randomness in output.
        /// Lower = deterministic, Higher = creative.
        /// Range: 0.0–2.0 (typical: 0.7–1.0).
        /// </summary>
        [JsonProperty("temperature")] public float? Temperature { get; set; }

        /// <summary>
        /// Penalizes repetition of recent tokens.
        /// Range: 0.0–2.0 (typical: 1.1).
        /// </summary>
        [JsonProperty("repeat_penalty")] public float? RepeatPenalty { get; set; }

        /// <summary>
        /// Penalizes tokens already present in the generated content.
        /// Range: -2.0–2.0 (typical: 0–1.0).
        /// </summary>
        [JsonProperty("presence_penalty")] public float? PresencePenalty { get; set; }

        /// <summary>
        /// Penalizes tokens that occur frequently across the generated content.
        /// Range: -2.0–2.0 (typical: 0–1.0).
        /// </summary>
        [JsonProperty("frequency_penalty")] public float? FrequencyPenalty { get; set; }

        /// <summary>
        /// List of strings that, if generated, will stop further generation.
        /// </summary>
        [JsonProperty("stop")] public List<string> Stop { get; set; }

        // Advanced sampling options (not in Ollama) ------------------------------------

        /// <summary>
        /// Biases specific tokens by ID. Use to influence token selection.
        /// Key = token ID (as string), Value = bias (-100 to 100, 0 = no bias).
        /// </summary>
        public Dictionary<string, double> LogitBias { get; set; }

        /// <summary>
        /// Number of top log probabilities to return per token.
        /// Range: 0–20 (if supported).
        /// </summary>
        public int? TopLogprobs { get; set; }

        /// <summary>
        /// OpenAI only. Whether to return log probabilities of the Output tokens or not. 
        /// If true, returns the log probabilities of each Output token returned in the content of message. 
        /// This option is currently not available on the gpt-4-vision-preview model.
        /// Defaults to 0.
        /// </summary>
        public int? Logprobs { get; set; }

        // Ollama-specific options ------------------------------------------------------

        /// <summary>
        /// Ollama only. Number of initial tokens to keep from context when truncating.
        /// </summary>
        [JsonProperty("num_keep")] public int? NumKeep { get; set; }

        /// <summary>
        /// Ollama only. Maximum number of tokens to predict (like max_tokens).
        /// </summary>
        [JsonProperty("num_predict")] public int? NumPredict { get; set; }

        /// <summary>
        /// Ollama only. Typical sampling, alternative to top_p.
        /// Range: 0.0–1.0
        /// </summary>
        [JsonProperty("typical_p")] public float? TypicalP { get; set; }

        /// <summary>
        /// Ollama only. Number of previous tokens to consider for repetition penalty.
        /// Typical: 64–256
        /// </summary>
        [JsonProperty("repeat_last_n")] public int? RepeatLastN { get; set; }

        /// <summary>
        /// Ollama only. Enables Mirostat sampling (0: off, 1 or 2: enabled).
        /// </summary>
        [JsonProperty("mirostat")] public int? Mirostat { get; set; }

        /// <summary>
        /// Ollama only. ontrols surprise level in Mirostat sampling.
        /// Typical: 5.0
        /// </summary>
        [JsonProperty("mirostat_tau")] public float? MirostatTau { get; set; }

        /// <summary>
        /// Ollama only. Controls learning rate in Mirostat sampling.
        /// Typical: 0.1
        /// </summary>
        [JsonProperty("mirostat_eta")] public float? MirostatEta { get; set; }

        /// <summary>
        /// Ollama only. Whether to apply penalties to newline tokens.
        /// </summary>
        [JsonProperty("penalize_newline")] public bool? PenalizeNewline { get; set; }

        /// <summary>
        /// Ollama only. Enable NUMA-aware optimization.
        /// </summary>
        [JsonProperty("numa")] public bool? Numa { get; set; }

        /// <summary>
        /// Ollama only. Number of context tokens (max sequence length).
        /// </summary>
        [JsonProperty("num_ctx")] public int? NumCtx { get; set; }

        /// <summary>
        /// Number of tokens to process in a single batch.
        /// </summary>
        [JsonProperty("num_batch")] public int? NumBatch { get; set; }

        /// <summary>
        /// Ollama only. Number of GPUs to use.
        /// </summary>
        [JsonProperty("num_gpu")] public int? NumGpu { get; set; }

        /// <summary>
        /// Ollama only. ID of the main GPU to prioritize.
        /// </summary>
        [JsonProperty("main_gpu")] public int? MainGpu { get; set; }

        /// <summary>
        /// Ollama only. Use VRAM-optimized loading.
        /// </summary>
        [JsonProperty("low_vram")] public bool? LowVram { get; set; }

        /// <summary>
        /// Ollama only. Only load the vocabulary; do not load full model.
        /// </summary>
        [JsonProperty("vocab_only")] public bool? VocabOnly { get; set; }

        /// <summary>
        /// Ollama only. Use memory-mapped files.
        /// </summary>
        [JsonProperty("use_mmap")] public bool? UseMmap { get; set; }

        /// <summary>
        /// Ollama only. Lock model in RAM.
        /// </summary>
        [JsonProperty("use_mlock")] public bool? UseMlock { get; set; }

        /// <summary>
        /// Ollama only. Number of CPU threads to use for inference.
        /// Typical: number of physical CPU cores.
        /// </summary>
        [JsonProperty("num_thread")] public int? NumThread { get; set; }

        public class Builder
        {
            private readonly ModelOptions _options = new();

            /// <summary>
            /// Sets the random seed for reproducible results.
            /// </summary>
            public Builder Seed(int seed) => Return(_options.Seed = seed);

            /// <summary>
            /// Limits next token sampling to top K tokens. (1–100)
            /// </summary>
            public Builder TopK(int topK) => Return(_options.TopK = rc.ClampWithWarning(topK, 1, 100, nameof(_options.TopK)));

            /// <summary>
            /// Controls cumulative probability sampling (top-p). Range: 0.0–1.0
            /// </summary>
            public Builder TopP(float topP) => Return(_options.TopP = rc.ClampWithWarning(topP, 0f, 1f, nameof(_options.TopP)));

            /// <summary>
            /// Filters out tokens with probability less than this. Range: 0.0–1.0
            /// </summary>
            public Builder MinP(float minP) => Return(_options.MinP = rc.ClampWithWarning(minP, 0f, 1f, nameof(_options.MinP)));

            /// <summary>
            /// Controls creativity/randomness. Lower = deterministic. Range: 0.0–2.0
            /// </summary>
            public Builder Temperature(float value) => Return(_options.Temperature = rc.ClampWithWarning(value, 0f, 2f, nameof(_options.Temperature)));

            /// <summary>
            /// Penalizes repetition of tokens. Higher = less repetition. Range: 0.0–2.0
            /// </summary>
            public Builder RepeatPenalty(float penalty) => Return(_options.RepeatPenalty = rc.ClampWithWarning(penalty, 0f, 2f, nameof(_options.RepeatPenalty)));

            /// <summary>
            /// Discourages use of already seen tokens. Range: -2.0–2.0
            /// </summary>
            public Builder PresencePenalty(float penalty) => Return(_options.PresencePenalty = rc.ClampWithWarning(penalty, -2f, 2f, nameof(_options.PresencePenalty)));

            /// <summary>
            /// Penalizes frequent tokens. Range: -2.0–2.0
            /// </summary>
            public Builder FrequencyPenalty(float penalty) => Return(_options.FrequencyPenalty = rc.ClampWithWarning(penalty, -2f, 2f, nameof(_options.FrequencyPenalty)));

            /// <summary>
            /// Stops generation when any of these strings appear.
            /// </summary>
            public Builder Stop(params string[] stopSequences) => Return(_options.Stop = new List<string>(stopSequences));

            /// <summary>
            /// Biases token selection by ID. Advanced use only.
            /// </summary>
            public Builder LogitBias(Dictionary<string, double> bias) => Return(_options.LogitBias = bias);

            /// <summary>
            /// Number of top log-probabilities to return per token. (0–20)
            /// </summary>
            public Builder TopLogprobs(int count) => Return(_options.TopLogprobs = rc.ClampWithWarning(count, 0, 20, nameof(_options.TopLogprobs)));

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Keeps first N tokens from truncation.
            /// </summary>
            public Builder NumKeep(int numKeep) => Return(_options.NumKeep = numKeep);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Maximum tokens to predict.
            /// </summary>
            public Builder NumPredict(int numPredict) => Return(_options.NumPredict = numPredict);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Alternative to top-p sampling. Range: 0.0–1.0
            /// </summary>
            public Builder TypicalP(float p) => Return(_options.TypicalP = rc.ClampWithWarning(p, 0f, 1f, nameof(_options.TypicalP)));

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Number of recent tokens to consider for repetition penalty. (0–1000)
            /// </summary>
            public Builder RepeatLastN(int n) => Return(_options.RepeatLastN = rc.ClampWithWarning(n, 0, 1000, nameof(_options.RepeatLastN)));

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Enables Mirostat sampling (0 = off, 1/2 = on).
            /// </summary>
            public Builder Mirostat(int value) => Return(_options.Mirostat = rc.ClampWithWarning(value, 0, 2, nameof(_options.Mirostat)));

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Controls surprise level in Mirostat.
            /// </summary>
            public Builder MirostatTau(float value) => Return(_options.MirostatTau = value);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Controls learning rate in Mirostat.
            /// </summary>
            public Builder MirostatEta(float value) => Return(_options.MirostatEta = value);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Whether to penalize newline tokens.
            /// </summary>
            public Builder PenalizeNewline(bool penalize) => Return(_options.PenalizeNewline = penalize);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Enables NUMA-aware optimization.
            /// </summary>
            public Builder Numa(bool useNuma) => Return(_options.Numa = useNuma);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Number of tokens in context window.
            /// </summary>
            public Builder NumCtx(int ctx) => Return(_options.NumCtx = ctx);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Number of tokens per batch.
            /// </summary>
            public Builder NumBatch(int batch) => Return(_options.NumBatch = batch);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Number of GPUs to use.
            /// </summary>
            public Builder NumGpu(int count) => Return(_options.NumGpu = count);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// ID of main GPU to prioritize.
            /// </summary>
            public Builder MainGpu(int id) => Return(_options.MainGpu = id);

            /// <summary> 
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Use VRAM-optimized mode.
            /// </summary>
            public Builder LowVram(bool enabled) => Return(_options.LowVram = enabled);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Load only vocabulary.
            /// </summary>
            public Builder VocabOnly(bool enabled) => Return(_options.VocabOnly = enabled);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Use memory-mapped files.
            /// </summary>
            public Builder UseMmap(bool enabled) => Return(_options.UseMmap = enabled);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Lock model into memory.
            /// </summary>
            public Builder UseMlock(bool enabled) => Return(_options.UseMlock = enabled);

            /// <summary>
            /// <see cref="AIProvider.Ollama"/>-only.
            /// Number of CPU threads to use.
            /// </summary>
            public Builder NumThread(int threadCount) => Return(_options.NumThread = threadCount);
            private Builder Return<T>(T _) => this;
            public ModelOptions Build() => _options;
        }
    }

    public class LocalModelOptionsConverter : JsonConverter<ModelOptions>
    {
        public override void WriteJson(JsonWriter writer, ModelOptions value, JsonSerializer serializer)
        {
            writer.WriteStartObject();

            var type = value.GetType();
            var props = type.GetProperties(BindingFlags.Instance | BindingFlags.Public);

            foreach (var prop in props)
            {
                if (!prop.CanRead) continue;

                var propValue = prop.GetValue(value);
                writer.WritePropertyName(prop.Name);
                serializer.Serialize(writer, propValue);
            }

            writer.WriteEndObject();
        }

        public override ModelOptions ReadJson(JsonReader reader, Type objectType, ModelOptions existingValue, bool hasExistingValue, JsonSerializer serializer)
        {
            var instance = existingValue ?? (ModelOptions)Activator.CreateInstance(objectType);
            serializer.Populate(reader, instance);
            return instance;
        }
    }

    public class OllamaModelOptionsConverter : JsonConverter<ModelOptions>
    {
        public override void WriteJson(JsonWriter writer, ModelOptions value, JsonSerializer serializer)
        {
            writer.WriteStartObject();

            var type = value.GetType();
            var props = type.GetProperties(BindingFlags.Instance | BindingFlags.Public);

            foreach (var prop in props)
            {
                var jsonAttr = prop.GetCustomAttribute<JsonPropertyAttribute>();
                if (jsonAttr == null) continue;

                var propertyName = jsonAttr.PropertyName ?? prop.Name;
                var propertyValue = prop.GetValue(value);

                writer.WritePropertyName(propertyName);
                serializer.Serialize(writer, propertyValue);
            }

            writer.WriteEndObject();
        }

        public override ModelOptions ReadJson(JsonReader reader, Type objectType, ModelOptions existingValue, bool hasExistingValue, JsonSerializer serializer)
        {
            var options = new ModelOptions();
            var type = options.GetType();

            while (reader.Read())
            {
                if (reader.TokenType == JsonToken.EndObject) break;

                if (reader.TokenType == JsonToken.PropertyName)
                {
                    var propertyName = reader.Value.ToString();
                    var prop = type.GetProperty(propertyName, BindingFlags.Instance | BindingFlags.Public);

                    if (prop != null)
                    {
                        reader.Read(); // Move to the value token
                        var value = serializer.Deserialize(reader, prop.PropertyType);
                        prop.SetValue(options, value);
                    }
                }
            }

            return options;
        }
    }
}
